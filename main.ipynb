{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.models as models\n",
    "\n",
    "# import steplr\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset.dataset import AVADataset\n",
    "\n",
    "from model.model import *\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# random seed \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.getenv(\"AVA_IMAGE_PATH\")\n",
    "TRAIN_CSV = os.getenv(\"AVA_TRAIN_CSV\")\n",
    "TEST_CSV  = os.getenv(\"AVA_TEST_CSV\")\n",
    "VAL_CSV   = os.getenv(\"AVA_VAL_CSV\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CONV_LEARNING_RATE = 5e-4\n",
    "DENSE_LEARNING_RATE = 5e-3\n",
    "\n",
    "# set conv to 3 × 10^−7\n",
    "# CONV_LEARNING_RATE = 3e-7\n",
    "# DENSE_LEARNING_RATE = 5e-6\n",
    "\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.9\n",
    "DECAY_FREQ = 1\n",
    "\n",
    "EPOCHS = 100\n",
    "EARLY_STOPPING = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = AVADataset(TRAIN_CSV, IMAGE_PATH, transform=train_transform)\n",
    "valset   = AVADataset(VAL_CSV, IMAGE_PATH, transform=val_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count())\n",
    "val_loader   = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epoch, train_losses, avg_loss):\n",
    "    # Train\n",
    "    batch_losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, data in enumerate(pbar):\n",
    "        images = data['image'].to(device)\n",
    "        labels = data['annotations'].to(device).float()\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.view(-1, 10, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = emd_loss(labels, outputs)\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('batch train loss', loss.data[0], i + epoch * (len(trainset) // BATCH_SIZE + 1))\n",
    "        \n",
    "        # tqdm description\n",
    "        pbar.set_description('Epoch: [{0}][{1}/{2}]\\t' 'Batch Loss {loss:.4f}\\t'.format(epoch + 1, i, len(trainset) // BATCH_SIZE + 1, loss=loss.data[0]))\n",
    "    \n",
    "    avg_loss = np.sum(batch_losses) / (len(trainset) // BATCH_SIZE + 1)\n",
    "    train_losses.append(avg_loss)\n",
    "    print('Epoch: [{0}]\\t' 'Mean Training EMD Loss {loss:.4f}\\t'.format(epoch + 1, loss=avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_losses, avg_val_loss, avg_loss, epoch):\n",
    "    # Validation\n",
    "    batch_val_losses = []\n",
    "    pbar = tqdm(val_loader)\n",
    "    for data in pbar:\n",
    "        images = data['image'].to(device)\n",
    "        labels = data['annotations'].to(device).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "        outputs = outputs.view(-1, 10, 1)\n",
    "        val_loss = emd_loss(labels, outputs)\n",
    "        batch_val_losses.append(val_loss.item())\n",
    "        \n",
    "        # tqdm description\n",
    "        pbar.set_description('Epoch: [{0}] Validation Loss {loss:.4f}\\t'.format(epoch + 1, loss=val_loss.data[0]))\n",
    "        \n",
    "    avg_val_loss = np.sum(batch_val_losses) / (len(valset) // BATCH_SIZE + 1)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print('Epoch: [{0}]\\t' 'Mean Validation EMD Loss {loss:.4f}\\t'.format(epoch + 1, loss=avg_val_loss))\n",
    "    writer.add_scalars('epoch loss', {'train': avg_loss, 'val': avg_val_loss}, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 27694154\n"
     ]
    }
   ],
   "source": [
    "base_model = models.vgg16(weights = models.VGG16_Weights.DEFAULT)\n",
    "model = NIMA(base_model).to(device)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.features.parameters(), 'lr': CONV_LEARNING_RATE},\n",
    "    {'params': model.classifier.parameters(), 'lr': DENSE_LEARNING_RATE}])\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "param_num = 0\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        param_num += param.numel()\n",
    "print('Total trainable parameters: %d' % param_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1][3118/7187]\tBatch Loss 0.0846\t:  43%|████▎     | 3119/7187 [09:50<12:49,  5.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m avg_val_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_model(model, optimizer, epoch, train_losses, avg_loss)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Decay\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# if DECAY:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#     if (epoch + 1) % 10 == 0:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m#             {'params': model.classifier.parameters(), 'lr': DENSE_LEARNING_RATE}],\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#             momentum=0.9)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epoch, train_losses, avg_loss)\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m writer\u001b[39m.\u001b[39;49madd_scalar(\u001b[39m'\u001b[39;49m\u001b[39mbatch train loss\u001b[39;49m\u001b[39m'\u001b[39;49m, loss\u001b[39m.\u001b[39;49mdata[\u001b[39m0\u001b[39;49m], i \u001b[39m+\u001b[39;49m epoch \u001b[39m*\u001b[39;49m (\u001b[39mlen\u001b[39;49m(trainset) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m BATCH_SIZE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     21\u001b[0m \u001b[39m# tqdm description\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pbar\u001b[39m.\u001b[39mset_description(\u001b[39m'\u001b[39m\u001b[39mEpoch: [\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m][\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBatch Loss \u001b[39m\u001b[39m{loss:.4f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, i, \u001b[39mlen\u001b[39m(trainset) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m BATCH_SIZE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m/media/research/neural-image-assessment/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:388\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m workspace\n\u001b[1;32m    386\u001b[0m     scalar_value \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mFetchBlob(scalar_value)\n\u001b[0;32m--> 388\u001b[0m summary \u001b[39m=\u001b[39m scalar(\n\u001b[1;32m    389\u001b[0m     tag, scalar_value, new_style\u001b[39m=\u001b[39;49mnew_style, double_precision\u001b[39m=\u001b[39;49mdouble_precision\n\u001b[1;32m    390\u001b[0m )\n\u001b[1;32m    391\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m/media/research/neural-image-assessment/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:280\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscalar\u001b[39m(name, tensor, collections\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, new_style\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, double_precision\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    265\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     tensor \u001b[39m=\u001b[39m make_np(tensor)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m    281\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    282\u001b[0m         tensor\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    283\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     \u001b[39m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m/media/research/neural-image-assessment/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/_convert_np.py:23\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([x])\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m _prepare_pytorch(x)\n\u001b[1;32m     24\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, but numpy array, torch tensor, or caffe2 blob name are expected.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     26\u001b[0m         \u001b[39mtype\u001b[39m(x)\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/media/research/neural-image-assessment/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/_convert_np.py:32\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_pytorch\u001b[39m(x):\n\u001b[0;32m---> 32\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "init_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    \n",
    "    # train\n",
    "    train_model(model, optimizer, epoch, train_losses, avg_loss)\n",
    "    \n",
    "    # Decay\n",
    "    # if DECAY:\n",
    "    #     if (epoch + 1) % 10 == 0:\n",
    "    #         CONV_LEARNING_RATE = CONV_LEARNING_RATE * DECAY_RATE ** ((epoch + 1) // DECAY_FREQ)\n",
    "    #         DENSE_LEARNING_RATE = DENSE_LEARNING_RATE * DECAY_RATE ** ((epoch + 1) // DECAY_FREQ)\n",
    "    #         optimizer = optim.SGD([\n",
    "    #             {'params': model.features.parameters(), 'lr': CONV_LEARNING_RATE},\n",
    "    #             {'params': model.classifier.parameters(), 'lr': DENSE_LEARNING_RATE}],\n",
    "    #             momentum=0.9)\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    # validate\n",
    "    validation(model, val_losses, avg_val_loss, avg_loss, epoch)\n",
    "    \n",
    "    if avg_val_loss < init_val_loss:\n",
    "        init_val_loss = avg_val_loss\n",
    "        \n",
    "        # save model\n",
    "        if not os.path.exists('checkpoints'):\n",
    "            os.makedirs('checkpoints')\n",
    "        torch.save(model.state_dict(), os.path.join('checkpoints', f'epoch_{epoch + 1}_val_loss_{avg_val_loss}.pth'))\n",
    "        print('Model saved')\n",
    "        count = 0\n",
    "    elif avg_val_loss >= init_val_loss:\n",
    "        count += 1\n",
    "        if count == EARLY_STOPPING:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
